{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 张量 (Tensor):\n",
    "数学中指的是多维数组；\n",
    "\n",
    "**`torch.Tensor`**\n",
    "```\n",
    "data: 被封装的 Tensor\n",
    "dtype: 张量的数据类型\n",
    "shape: 张量的形状\n",
    "device: 张量所在的设备，GPU/CPU\n",
    "requires_grad: 指示是否需要计算梯度\n",
    "grad: data 的梯度\n",
    "grad_fn: 创建 Tensor 的 Function，是自动求导的关键\n",
    "is_leaf: 指示是否是叶子结点 (叶子结点指的是用户创建的节点，比如 y=(x+w)*(w+1)中，x和w就是叶子结点【可以用计算图来清楚地表示该过程】)\n",
    "```\n",
    "\n",
    "# 2. 创建张量:\n",
    "## 2.1 直接创建:\n",
    "**2.1.1 从 data 创建 tensor**\n",
    "```\n",
    "data: 可以是 list, numpy\n",
    "dtype: 数据类型，默认与data一致\n",
    "device: 所在设备\n",
    "requires_grad: 是否需要梯度\n",
    "pin_memory: 是否存于锁页内存\n",
    "```\n",
    "```python\n",
    "# 创建方法\n",
    "torch.tensor(\n",
    "    data,\n",
    "    dtype=None,\n",
    "    device=None,\n",
    "    requires_grad=False,\n",
    "    pin_memory=False\n",
    ")\n",
    "```\n",
    "\n",
    "**2.1.2 从 numpy 创建 tensor**\n",
    "\n",
    "<font color=\"red\">用`torch.from_numpy()`创建的tensor与原始的ndarray共享内存，修改其中一个，另一个也会变。</font>\n",
    "\n",
    "```python\n",
    "# 创建方法\n",
    "torch.from_numpy(ndarray)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='mps:0')\n",
      "torch.int64\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 举例\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 直接创建\n",
    "## torch.tensor()\n",
    "arr = np.ones((3,3))\n",
    "t = torch.tensor(arr, \n",
    "                 dtype=torch.float32, \n",
    "                 device=\"mps\") # 把张量放到 GPU 上 (mac M1)\n",
    "\n",
    "print(t.dtype)\n",
    "print(t)\n",
    "\n",
    "\n",
    "## torch.from_numpy()\n",
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "t = torch.from_numpy(arr)\n",
    "print(t.dtype)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 根据数值进行创建:\n",
    "\n",
    "**2.2.1 `torch.zeros()`**\n",
    "\n",
    "创建全0张量\n",
    "\n",
    "```\n",
    "size: 张量的形状\n",
    "out: 输出的张量 (暂时可以不考虑)\n",
    "layout:内存中布局形式，有 strided (通常情况下使用), sparse_coo (读取稀疏矩阵会用到)\n",
    "device: 所在设备\n",
    "requires_grad: 是否需要梯度\n",
    "```\n",
    "\n",
    "```python\n",
    "# 创建方法\n",
    "torch.zeros(\n",
    "    *size,\n",
    "    out=None,\n",
    "    dtype=None,\n",
    "    layout=torch.strided,\n",
    "    device=False,\n",
    "    requires_grad=False\n",
    ")\n",
    "```\n",
    "\n",
    "**2.2.2 `torch.zeros_lisk()`**\n",
    "\n",
    "根据 input 形状创建全0张量\n",
    "\n",
    "```\n",
    "input: 创建与 input 同形状的张量；\n",
    "dtype: 数据类型；\n",
    "layout: 内存中的布局形式；\n",
    "```\n",
    "\n",
    "```python\n",
    "# 创建方法\n",
    "torch.zeros_like(\n",
    "    input,\n",
    "    dtype=None,\n",
    "    layout=None,\n",
    "    device=None,\n",
    "    requires_grad=False\n",
    ")\n",
    "```\n",
    "\n",
    "**2.2.3 `torch.ones()` 和 `torch.ones_like()`**\n",
    "\n",
    "创建全1张量\n",
    "\n",
    "\n",
    "**2.2.4 `torch.full()`和`torch.full_like()`**\n",
    "\n",
    "创建自定义数值的张量\n",
    "\n",
    "```\n",
    "size: 张量形状\n",
    "fill_value: 张量的值\n",
    "```\n",
    "\n",
    "**2.2.5 `torch.arange()`**\n",
    "\n",
    "根据数列创建等差1维张量，[start, end)\n",
    "\n",
    "```\n",
    "start: 起始值\n",
    "end: 结束值\n",
    "step: 数列公差，默认为1\n",
    "```\n",
    "\n",
    "**2.2.6`torch.linspace()`**\n",
    "\n",
    "创建均分的1维张量，[start, end]\n",
    "\n",
    "```\n",
    "start: 起始值\n",
    "end: 结束值\n",
    "step: 数列长度\n",
    "```\n",
    "\n",
    "**2.2.7`torch.logspace()`**\n",
    "\n",
    "创建对数均分的1D张量\n",
    "\n",
    "```\n",
    "start: 起始值\n",
    "end: 结束值\n",
    "steps: 数列长度\n",
    "base: 对数函数的底，默认为10\n",
    "```\n",
    "\n",
    "```python\n",
    "# 创建方法\n",
    "torch.logspace(\n",
    "    start,\n",
    "    end,\n",
    "    steps=100,\n",
    "    base=10.0,\n",
    "    out=None,\n",
    "    dtype=None,\n",
    "    layout=torch.strided,\n",
    "    deivce=None,\n",
    "    requires_grad=False\n",
    ")\n",
    "```\n",
    "\n",
    "**2.2.8 `torch.eye()`**\n",
    "\n",
    "创建单位对角矩阵 (2D张量)\n",
    "\n",
    "```\n",
    "n: 矩阵行数\n",
    "m: 矩阵列数\n",
    "```\n",
    "\n",
    "```python\n",
    "# 创建方法\n",
    "torch.eye(\n",
    "    n,\n",
    "    m=None,\n",
    "    out=None,\n",
    "    dtype=None,\n",
    "    layout=torch.strided,\n",
    "    device=None,\n",
    "    requires_grad=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]])\n",
      "tensor([2, 4, 6])\n",
      "tensor([ 2.,  4.,  6.,  8., 10.])\n"
     ]
    }
   ],
   "source": [
    "# 举例\n",
    "t = torch.zeros((3,3))\n",
    "print(t.dtype)\n",
    "print(t)\n",
    "\n",
    "t = torch.full((3,3),2) # 创建3x3的全2张量\n",
    "print(t)\n",
    "\n",
    "t = torch.arange(2, 8, 2)\n",
    "print(t)\n",
    "\n",
    "t = torch.linspace(2, 10, 5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 根据概率分布创建张量:\n",
    "**2.3.1 `torch.normal()`**\n",
    "\n",
    "生成正态分布（高斯分布）\n",
    "\n",
    "四种模式：\n",
    "|||||\n",
    "|--|--|--|--|\n",
    "|mean|标量|std|标量|\n",
    "|mean|标量|std|张量|\n",
    "|mean|张量|std|标量|\n",
    "|mean|张量|std|张量|\n",
    "\n",
    "\n",
    "```python\n",
    "# 张量\n",
    "torch.normal(\n",
    "    mean,\n",
    "    std,\n",
    "    out=None\n",
    ")\n",
    "\n",
    "# 标量\n",
    "torch.normal(\n",
    "    mean,\n",
    "    std,\n",
    "    size,\n",
    "    out=None\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "**2.3.2 `torch.randn()`, `torch.randn_like()`**\n",
    "\n",
    "生成标准正态分布\n",
    "\n",
    "```python\n",
    "torch.randn(\n",
    "    *size, ## 张量形状\n",
    "    out=None,\n",
    "    dtype=None,\n",
    "    layout=torch.strided,\n",
    "    device=None,\n",
    "    requires_grad=False\n",
    ")\n",
    "```\n",
    "\n",
    "**2.3.3 `torch.rand()`, `torch.rand_like()`**\n",
    "\n",
    "在[0,1)区间上生成均匀分布\n",
    "\n",
    "```python\n",
    "torch.rand(\n",
    "    *size, ## 张量形状\n",
    "    out=None,\n",
    "    dtype=None,\n",
    "    layout=torch.strided,\n",
    "    device=None,\n",
    "    requires_grad=False\n",
    ")\n",
    "```\n",
    "\n",
    "**2.3.4 `torch.randint()`, `torch.randint_like()`**\n",
    "\n",
    "在 [low, high) 区间上生成整数均匀分布\n",
    "\n",
    "```python\n",
    "torch.randint(\n",
    "    low,\n",
    "    high,\n",
    "    size,\n",
    "    out=None,\n",
    "    dtype=None,\n",
    "    layout=torch.strided,\n",
    "    device=None,\n",
    "    requires_grad=False\n",
    ")\n",
    "```\n",
    "\n",
    "**2.3.5 `torch.randperm()`**\n",
    "\n",
    "生成0到n-1的随机排列\n",
    "\n",
    "```\n",
    "n: 张量长度\n",
    "``````\n",
    "\n",
    "```python\n",
    "torch.randperm(\n",
    "    n,\n",
    "    out=None,\n",
    "    dtype=torch.int64,\n",
    "    layout=torch.strided,\n",
    "    device=None,\n",
    "    requires_grad=False\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "**2.3.6`torch.bernoulli()`**\n",
    "\n",
    "以 input 为概率，生成伯努利分布(0-1分布，两点分布)\n",
    "\n",
    "```python\n",
    "torch.bernoulli(\n",
    "    input,\n",
    "    *,\n",
    "    generator=None,\n",
    "    out=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 张量的操作：\n",
    "\n",
    "## 张量的操作：拼接、切分、索引和变换：\n",
    "\n",
    "### 1. 拼接：\n",
    "**`1.1 torch.cat()`**\n",
    "\n",
    "将张量按维度 dim 进行拼接 (不回扩张张量维度)\n",
    "\n",
    "```python\n",
    "torch.cat(\n",
    "    tensors, ## 张量序列\n",
    "    dim, ## 要拼接的维度\n",
    "    out=None\n",
    ")\n",
    "```\n",
    "\n",
    "**1.2 `torch.stack()`**\n",
    "\n",
    "在新创建的维度 dim 上进行拼接 (会扩张张量的维度)\n",
    "\n",
    "```python\n",
    "torch.stack(\n",
    "    tensors, ## 张量序列\n",
    "    dim=0, ## 要拼接的维度\n",
    "    out=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 shape:  torch.Size([4, 3])\n",
      "t2 shape:  torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.ones((2,3))\n",
    "\n",
    "# torch.cat()\n",
    "t1 = torch.cat([t,t], dim=0) ## 按照 dim=0 对t进行拼接，得到(4,3)\n",
    "\n",
    "# torch.stack()\n",
    "t2 = torch.stack([t,t], dim=0) ## 按照 dim=0 对t进行拼接，得到(2,2,3)，会在dim=0创建一个新维度\n",
    "\n",
    "print(\"t1 shape: \", t1.shape)\n",
    "print(\"t2 shape: \", t2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 切分\n",
    "\n",
    "**2.1 `torch.chunk()`**\n",
    "\n",
    "将张量按照维度 dim 进行平均切分，返回张量列表 (如果不能整除，最后一份张量小于其他张量)\n",
    "\n",
    "```python\n",
    "torch.chunk(\n",
    "    input, ## 要切分的张量\n",
    "    chunks, ## 要切分的份数\n",
    "    dim=0 ## 要切分的维度\n",
    ")\n",
    "```\n",
    "\n",
    "**2.2 `torch.split()`**\n",
    "\n",
    "将张量按照维度 dim 进行切分，返回张量列表\n",
    "\n",
    "```python\n",
    "torch.split(\n",
    "    tensor, ## 要切分的张量\n",
    "    split_size_of_sections, ## 为 int 时，表示每一份的长度；为 list 时，按 list 元素切分\n",
    "    dim=0 ## 要切分的维度\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1, shape is torch.Size([2, 3])\n",
      "order 2, shape is torch.Size([2, 2])\n",
      "order 1, shape is torch.Size([2, 2])\n",
      "order 2, shape is torch.Size([2, 1])\n",
      "order 3, shape is torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones((2,5))\n",
    "\n",
    "# torch.chunk()\n",
    "list_of_tensors = torch.chunk(t1, dim=1, chunks=2) ## 5不能被2整除，所以最后一个张量形状小雨前面的张量\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"order {}, shape is {}\".format(idx+1, t.shape))\n",
    "\n",
    "# torch.split()\n",
    "list_of_tensors = torch.split(t1, [2,1,2], dim=1)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"order {}, shape is {}\".format(idx+1, t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 索引\n",
    "\n",
    "**3.1 `torch.index_select()`**\n",
    "\n",
    "在维度 dim 上，按照 index 索引数据，返回依index索引数据拼接的张量 (先索引，再拼接)\n",
    "\n",
    "```python\n",
    "torch.index_select(\n",
    "    input, ## 要索引的张量\n",
    "    dim, ## 要索引的维度\n",
    "    index, ## 要索引数据的序号，数据类型必须是 torch.long\n",
    "    out=None\n",
    ")\n",
    "```\n",
    "\n",
    "**3.2 `torch.masked_select()`**\n",
    "\n",
    "按照 mask 中的 True 进行索引，返回一维张量\n",
    "\n",
    "```python\n",
    "torch.masked_select(\n",
    "    input,\n",
    "    mask, ## 与 input 同形状的布尔类型张量\n",
    "    out=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 8, 1],\n",
      "        [6, 0, 0],\n",
      "        [7, 6, 8]])\n",
      "tensor([[3, 1],\n",
      "        [6, 0],\n",
      "        [7, 8]])\n",
      "tensor([[3, 8, 1],\n",
      "        [6, 0, 0],\n",
      "        [7, 6, 8]])\n",
      "tensor([[False,  True, False],\n",
      "        [ True, False, False],\n",
      "        [ True,  True,  True]])\n",
      "tensor([8, 6, 7, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# torch_index_select()\n",
    "t = torch.randint(0,9,size=(3,3))\n",
    "idx = torch.tensor([0,2],dtype=torch.long)\n",
    "t_select = torch.index_select(t, dim=1, index=idx)\n",
    "print(t)\n",
    "print(t_select)\n",
    "\n",
    "# torch.masked_select()\n",
    "mask = t.ge(5) ## 表示张量中 >= 5的元素；ge()表示大于等于；gt()表示大于；le()表示小于等于；lt()表示小于\n",
    "t_select = torch.masked_select(t, mask=mask)\n",
    "print(t)\n",
    "print(mask)\n",
    "print(t_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 变换\n",
    "\n",
    "**4.1 `torch.reshape()`**\n",
    "\n",
    "变换张量形状 (张量在内存中是连续时，新张量与input共享数据内存)\n",
    "\n",
    "```python\n",
    "torch.reshape(\n",
    "    input,\n",
    "    shape ## 新张量的形状\n",
    ")\n",
    "```\n",
    "\n",
    "**4.2 `torch.transpose()`**\n",
    "\n",
    "交换张量的两个维度\n",
    "\n",
    "```python\n",
    "torch.transpose(\n",
    "    input,\n",
    "    dim0,\n",
    "    dim1\n",
    ")\n",
    "```\n",
    "\n",
    "**4.3 `torch.t()`**\n",
    "\n",
    "2维张量转置\n",
    "\n",
    "```python\n",
    "torch.t(input)\n",
    "```\n",
    "\n",
    "**4.4 `torch.squeeze()`**\n",
    "\n",
    "压缩长度为1的维度\n",
    "\n",
    "```python\n",
    "torch.squeeze(\n",
    "    input,\n",
    "    dim=None, ## 若 dim=None，则移除所有长度为1的轴；若指定维度而且维度长度为1，则可以移除该维度\n",
    "    out=None\n",
    ")\n",
    "```\n",
    "\n",
    "**4.5 `torch.unsqueeze()`**\n",
    "\n",
    "根据 dim 扩展维度\n",
    "\n",
    "```python\n",
    "torch.unsqueeze(\n",
    "    input,\n",
    "    dim,\n",
    "    out=None\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 4, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([1, 2, 3, 1])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "## torch.reshape\n",
    "\n",
    "t = torch.randperm(8)\n",
    "t_reshape = torch.reshape(t, (2,4)) ## 如果是 (-1,2,2)，那么-1表示的是：不需要关心-1处的维度是多少，根据后两个维度进行计算得到(比如：这里就是 8/2/2 = 2，即-1处的维度是2)\n",
    "\n",
    "print(t.shape)\n",
    "print(t_reshape.shape)\n",
    "\n",
    "\n",
    "## torch.transpose()\n",
    "t = torch.rand((2,3,4))\n",
    "t_transpose = torch.transpose(t, dim0=1, dim1=2) ## 把第1维和第2维进行交换\n",
    "print(t.shape)\n",
    "print(t_transpose.shape)\n",
    "\n",
    "\n",
    "## torch.squeeze()\n",
    "t = torch.rand((1,2,3,1))\n",
    "t_squeeze = torch.squeeze(t)\n",
    "t_squeeze_0 = torch.squeeze(t,dim=0)\n",
    "t_squeeze_1 = torch.squeeze(t,dim=1)\n",
    "print(t_squeeze.shape)\n",
    "print(t_squeeze_0.shape)\n",
    "print(t_squeeze_1.shape)\n",
    "\n",
    "\n",
    "## torch.unsqueeze()\n",
    "t = torch.rand((2,3))\n",
    "t_unsqueeze = torch.unsqueeze(t, dim=2)\n",
    "print(t_unsqueeze.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 张量的数学运算：\n",
    "\n",
    "### 加减乘除\n",
    "```python\n",
    "torch.add()\n",
    "torch.addcdiv()\n",
    "torch.addcmul()\n",
    "torch.sub()\n",
    "torch.div()\n",
    "torch.mul()\n",
    "```\n",
    "\n",
    "**重点: `torch.add()`和`torch.addcmul()`**\n",
    "\n",
    "`torch.add()`: 逐元素计算 input + alpha x other\n",
    "\n",
    "```python\n",
    "torch.add(\n",
    "    input, ## 第一个张量\n",
    "    alpha=1, ## 乘项因子 (alpha x other + input)\n",
    "    other, ## 第二个张量\n",
    "    out=None\n",
    ")\n",
    "```\n",
    "\n",
    "`torch.addcmul()`\n",
    "\n",
    "out = input + value x tensor1 x tensor2\n",
    "\n",
    "```python\n",
    "torch.addcmul(\n",
    "    input,\n",
    "    value=1.\n",
    "    tensor1,\n",
    "    tensor2,\n",
    "    out=None\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "### 对数、指数、幂函数\n",
    "```python\n",
    "torch.log(input, out=None)\n",
    "torch.log10(input, out=None)\n",
    "torch.log2(input, out=None)\n",
    "torch.exp(input, out=None)\n",
    "torch.pow()\n",
    "```\n",
    "\n",
    "\n",
    "### 三角函数\n",
    "```python\n",
    "torch.abs(input, out=None)\n",
    "torch.acos(input, out=None)\n",
    "torch.cosh(input, out=None)\n",
    "torch.cos(input, out=None)\n",
    "torch.asin(input, out=None)\n",
    "torch.atan(input, out=None)\n",
    "torch.atan2(input, other=None, out=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 计算图与动态图\n",
    "\n",
    "## 1. 计算图\n",
    "\n",
    "> 计算图是用来描述运算的有向无环图，包括两个主要元素：节点和边。节点表示数据（比如 向量、矩阵、张量）；边表示运算（比如 加减乘除卷积等）。\n",
    "\n",
    "> **叶子结点：指的是由用户创建的节点，其他非叶子结点都是由叶子结点通过直接或间接的运算得到的。（之所以会有叶子结点的概念，是因为梯度反向传播结束后，只有叶子结点的梯度会被保留，非叶子结点的梯度会被从内存中释放掉。可以通过`.is_leaf()`方法查看是否为叶子结点）。**\n",
    "\n",
    "> **如果想要使用非叶子结点的梯度，实现方法是 在计算梯度之后，用`.retain_grad()`方法保存非叶子结点的梯度。**\n",
    "\n",
    "**`grad_fn`: 记录创建该张量(非叶子结点)时所用的方法，比如 y=a*b，那么 `y.grad_fn = <MulBackward0>`**\n",
    "\n",
    "\n",
    "## 2. 动态图\n",
    "\n",
    "计算图可以根据搭建方式的不同，分为：动态图 (运算与搭建同时进行，灵活，易调节，比如 pytorch) 和静态图 (先搭建，后运算，高效，不灵活，比如 tensorflow)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 自动求导系统 `autograd`\n",
    "\n",
    "## 1. `torch.autograd.backward`\n",
    "\n",
    "自动计算图中各个结点的梯度\n",
    "\n",
    "```python\n",
    "torch.autograd.backward(\n",
    "    tensors, ## 用于求导的张量（比如 loss）\n",
    "    grad_tensors=None, ## 多梯度权重\n",
    "    retain_grap=None, ## 保存计算图\n",
    "    create_graph=False ## 创建导数的计算图，同于高阶求导\n",
    ")\n",
    "```\n",
    "\n",
    "## 2. `torch.autograd.grad`\n",
    "\n",
    "求取梯度\n",
    "\n",
    "```python\n",
    "torch.autograd.grad(\n",
    "    outputs, ## 用于求导的张量 (比如 loss)\n",
    "    inputs, ## 需要梯度的张量\n",
    "    grad_outputs=None, ## 多梯度权重 \n",
    "    retain_graph=None, ## 保存计算图\n",
    "    create_graph=False ## 创建导数计算图，用于高阶求导\n",
    ")\n",
    "```\n",
    "\n",
    "## 3. Pytorch 自动求导系统中有3个需要注意的点：\n",
    "> 1. 梯度不自动清零 (手动清零 `.grad.zero_()`)；\n",
    ">\n",
    "> 2. 依赖于叶子结点结点，requires_grad 默认为 True；\n",
    ">\n",
    "> 3. 叶子结点不可执行 in-place，即叶子结点不能执行原位操作；\n",
    "\n",
    "## 4. 机器学习模型的5个训练步骤：\n",
    "数据, 模型, 损失函数, 优化器, 迭代训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DataLoader 与 Dataset\n",
    "```\n",
    "数据\n",
    "    数据收集\n",
    "\n",
    "    数据划分\n",
    "\n",
    "    数据读取\n",
    "        DataLoader\n",
    "            Sampler: 生成索引\n",
    "            Dataset: 根据索引读取样本特征以及标签\n",
    "\n",
    "    数据预处理\n",
    "```\n",
    "\n",
    "## 1. `torch.utils.data.DataLoader`\n",
    "\n",
    "```python\n",
    "DataLoader(\n",
    "    dataset, ## Dataset类，决定数据从哪里读取以及如何读取\n",
    "    batch_size=1, ## 批量大小\n",
    "    shuffle=False, ## 每个 epoch 是否打乱顺序\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0, ## 是否多进程读取数据\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    "    drop_last=False, ## 当样本数不能被 batchsize 整除时，是否舍弃最后一批数据\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    multiprocessing_context=None\n",
    ")\n",
    "```\n",
    "\n",
    "## 2. `torch.utils.data.Dataset`\n",
    "\n",
    "Dataset抽象类，所有自定义的Dataset都需要继承它，并且需要复写 `__getitem__()`方法；`getitem`：接收一个索引，返回一个样本。\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "\n",
    "## Example: 猫狗图片分类 (\"./example_week1_Cat_Dog_classification/\")\n",
    "**1. 参考代码：https://github.com/JansonYuan/Pytorch-Camp/**\n",
    "\n",
    "**2. 猫狗数据集下载：https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification，但是将其中Test随机分成相等的两份，一份作为验证集，一份作为测试集（尽管测试集没有用到）。**\n",
    "\n",
    "**3. 代码在 \"./example_week1_Cat_Dog_classification/\" 目录下。**\n",
    "\n",
    "**4. 结果讨论：**\n",
    "\n",
    "4.1 作者原来的代码的结果：\n",
    "\n",
    "![](./Figures/week1_RMB.png)\n",
    "\n",
    "```\n",
    "Training:Epoch[000/010] Iteration[010/010] Loss: 0.6204 Acc:60.62%\n",
    "Valid:\t Epoch[000/010] Iteration[002/002] Loss: 0.4436 Acc:90.00%\n",
    "Training:Epoch[001/010] Iteration[010/010] Loss: 0.3601 Acc:90.62%\n",
    "Valid:\t Epoch[001/010] Iteration[002/002] Loss: 0.0461 Acc:100.00%\n",
    "Training:Epoch[002/010] Iteration[010/010] Loss: 0.0788 Acc:98.12%\n",
    "Valid:\t Epoch[002/010] Iteration[002/002] Loss: 0.0234 Acc:100.00%\n",
    "Training:Epoch[003/010] Iteration[010/010] Loss: 0.0208 Acc:99.38%\n",
    "Valid:\t Epoch[003/010] Iteration[002/002] Loss: 0.0022 Acc:100.00%\n",
    "Training:Epoch[004/010] Iteration[010/010] Loss: 0.0042 Acc:100.00%\n",
    "Valid:\t Epoch[004/010] Iteration[002/002] Loss: 0.0018 Acc:100.00%\n",
    "Training:Epoch[005/010] Iteration[010/010] Loss: 0.0143 Acc:99.38%\n",
    "Valid:\t Epoch[005/010] Iteration[002/002] Loss: 0.0002 Acc:100.00%\n",
    "Training:Epoch[006/010] Iteration[010/010] Loss: 0.0220 Acc:98.75%\n",
    "Valid:\t Epoch[006/010] Iteration[002/002] Loss: 0.0004 Acc:100.00%\n",
    "Training:Epoch[007/010] Iteration[010/010] Loss: 0.0008 Acc:100.00%\n",
    "Valid:\t Epoch[007/010] Iteration[002/002] Loss: 0.0013 Acc:100.00%\n",
    "Training:Epoch[008/010] Iteration[010/010] Loss: 0.0083 Acc:99.38%\n",
    "Valid:\t Epoch[008/010] Iteration[002/002] Loss: 0.0001 Acc:100.00%\n",
    "Training:Epoch[009/010] Iteration[010/010] Loss: 0.0049 Acc:100.00%\n",
    "Valid:\t Epoch[009/010] Iteration[002/002] Loss: 0.0000 Acc:100.00%\n",
    "```\n",
    "\n",
    "4.2 用 \"./example_week1_Cat_Dog_classification/train_lenet.py\" 代码处理 Pytorch-Camp 的结果和上面的是一致的（代码修改后，与 Pytorch-Camp 提供的代码可以对 RMB 实现同样的分类效果），如下所示：\n",
    "\n",
    "\n",
    "![](./Figures/week_1_RMB_diy_code.png)\n",
    "\n",
    "```\n",
    "Train:   Epoch[000/010] Iteration[010/010] Loss: 0.6204 Acc: 60.62%\n",
    "Valid:   Epoch[000/010] Iteration[002/002] Loss: 0.4436 Acc: 90.00%\n",
    "Train:   Epoch[001/010] Iteration[010/010] Loss: 0.3601 Acc: 90.62%\n",
    "Valid:   Epoch[001/010] Iteration[002/002] Loss: 0.0461 Acc: 100.00%\n",
    "Train:   Epoch[002/010] Iteration[010/010] Loss: 0.0788 Acc: 98.12%\n",
    "Valid:   Epoch[002/010] Iteration[002/002] Loss: 0.0234 Acc: 100.00%\n",
    "Train:   Epoch[003/010] Iteration[010/010] Loss: 0.0208 Acc: 99.38%\n",
    "Valid:   Epoch[003/010] Iteration[002/002] Loss: 0.0022 Acc: 100.00%\n",
    "Train:   Epoch[004/010] Iteration[010/010] Loss: 0.0042 Acc: 100.00%\n",
    "Valid:   Epoch[004/010] Iteration[002/002] Loss: 0.0018 Acc: 100.00%\n",
    "Train:   Epoch[005/010] Iteration[010/010] Loss: 0.0143 Acc: 99.38%\n",
    "Valid:   Epoch[005/010] Iteration[002/002] Loss: 0.0002 Acc: 100.00%\n",
    "Train:   Epoch[006/010] Iteration[010/010] Loss: 0.0220 Acc: 98.75%\n",
    "Valid:   Epoch[006/010] Iteration[002/002] Loss: 0.0004 Acc: 100.00%\n",
    "Train:   Epoch[007/010] Iteration[010/010] Loss: 0.0008 Acc: 100.00%\n",
    "Valid:   Epoch[007/010] Iteration[002/002] Loss: 0.0013 Acc: 100.00%\n",
    "Train:   Epoch[008/010] Iteration[010/010] Loss: 0.0083 Acc: 99.38%\n",
    "Valid:   Epoch[008/010] Iteration[002/002] Loss: 0.0001 Acc: 100.00%\n",
    "Train:   Epoch[009/010] Iteration[010/010] Loss: 0.0049 Acc: 100.00%\n",
    "Valid:   Epoch[009/010] Iteration[002/002] Loss: 0.0000 Acc: 100.00%\n",
    "```\n",
    "\n",
    "4.3 将数据换成猫狗数据集之后，结果如下：\n",
    "\n",
    "![](./Figures/week1_CatsDogs_diy_code.png)\n",
    "\n",
    "```\n",
    "Train:   Epoch[000/010] Iteration[035/035] Loss: 0.7151 Acc: 53.50%\n",
    "Valid:   Epoch[000/010] Iteration[005/005] Loss: 0.6898 Acc: 51.43%\n",
    "Train:   Epoch[001/010] Iteration[035/035] Loss: 0.6954 Acc: 50.09%\n",
    "Valid:   Epoch[001/010] Iteration[005/005] Loss: 0.7321 Acc: 41.43%\n",
    "Train:   Epoch[002/010] Iteration[035/035] Loss: 0.6885 Acc: 53.86%\n",
    "Valid:   Epoch[002/010] Iteration[005/005] Loss: 0.6954 Acc: 52.86%\n",
    "Train:   Epoch[003/010] Iteration[035/035] Loss: 0.6813 Acc: 58.53%\n",
    "Valid:   Epoch[003/010] Iteration[005/005] Loss: 0.6909 Acc: 55.71%\n",
    "Train:   Epoch[004/010] Iteration[035/035] Loss: 0.6832 Acc: 57.99%\n",
    "Valid:   Epoch[004/010] Iteration[005/005] Loss: 0.6749 Acc: 52.86%\n",
    "Train:   Epoch[005/010] Iteration[035/035] Loss: 0.6838 Acc: 58.35%\n",
    "Valid:   Epoch[005/010] Iteration[005/005] Loss: 0.6865 Acc: 50.00%\n",
    "Train:   Epoch[006/010] Iteration[035/035] Loss: 0.6729 Acc: 58.89%\n",
    "Valid:   Epoch[006/010] Iteration[005/005] Loss: 0.7097 Acc: 57.14%\n",
    "Train:   Epoch[007/010] Iteration[035/035] Loss: 0.6700 Acc: 61.58%\n",
    "Valid:   Epoch[007/010] Iteration[005/005] Loss: 0.7361 Acc: 47.14%\n",
    "Train:   Epoch[008/010] Iteration[035/035] Loss: 0.6721 Acc: 59.07%\n",
    "Valid:   Epoch[008/010] Iteration[005/005] Loss: 0.7180 Acc: 52.86%\n",
    "Train:   Epoch[009/010] Iteration[035/035] Loss: 0.6476 Acc: 63.91%\n",
    "Valid:   Epoch[009/010] Iteration[005/005] Loss: 0.7193 Acc: 57.14%\n",
    "```\n",
    "\n",
    "**结论：** \n",
    "\n",
    "**1.直接将 Pytorch-Camp 处理 RMB 分类问题时用到的 LeNet模型用于猫狗数据集分类，从结果来看是不合适的，可能需要对 LeNet进行调整（我猜后面可能会学到该符合调整网络结构）。**\n",
    "\n",
    "**2.从这个例子中进一步理解了训练一个神经网络的过程：数据、模型、损失函数、优化器、训练，同时也了解了每个过程中所涉及到的细节的处理。**\n",
    "\n",
    "**3.目前存在的问题就是：和第1点中提到的一样，对于不同的实际问题，该如何构建合理的深度学习网络（有哪些需要微调的地方？为什么在这个地方进行微调、有什么依据？）。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 数据预处理模块 —— transforms\n",
    "\n",
    "<font color='red' size=5>**因为不搞计算机视觉，所以这 transforms 这部分的内容暂时不学习...**</font>\n",
    "\n",
    "```\n",
    "torchvision: 计算机视觉工具包；\n",
    "\n",
    "torchvision.transforms: 常用的图像预处理方法；\n",
    "torchvision.datasets: 常用数据集的dataset实现，比如 MNIST, CIFAR-10, ImageNet等；\n",
    "torchvision.model: 常用的模型预训练，比如 AlexNet, VGG, ResNet, GoogLeNet等；\n",
    "```\n",
    "\n",
    "## torchvision.transforms\n",
    "数据中心化，标准化，缩放，裁剪，旋转，填充，翻转，噪声添加，灰度变换，线性变换，仿射变换，亮度、饱和度及对比对变换。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 模型\n",
    "\n",
    "```\n",
    "模型 (nn.Module)\n",
    "|--模型创建\n",
    "|   |--构建网络层\n",
    "|   |   |--卷积层、池化层、激活函数层等\n",
    "|   |--拼接网络层\n",
    "|   |   |--LeNet、AlexNet、ResNet等\n",
    "|--模型初始化\n",
    "|   |--Xavier、Kaiming、均匀分布、正态分布等\n",
    "```\n",
    "\n",
    "## 1.1 模型创建 (以 `LeNet` 为例)\n",
    "\n",
    "### 1). 构建子模块 `__init__()`\n",
    "```python\n",
    "def __init__(self, classes):\n",
    "    super(LeNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16*5*5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, classes)\n",
    "```\n",
    "\n",
    "### 2). 拼接子模块 `forward()`\n",
    "```python\n",
    "def forward(self, x):\n",
    "    out = F.relu(self.conv1(x))\n",
    "    out = F.max_pool2d(out, 2)\n",
    "\n",
    "    out = F.relu(self.conv2(x))\n",
    "    out = F.max_pool2d(out, 2)\n",
    "\n",
    "    out = out.view(out.size(0), -1)\n",
    "    \n",
    "    out = F.relu(self.fc1(out))\n",
    "    out = F.relu(self.fc2(out))\n",
    "    out = self.fc3(out)\n",
    "    \n",
    "    return out\n",
    "```\n",
    "\n",
    "### 综上，构建 `LeNet` 的整个过程：\n",
    "```python\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet, self).__init__() ## super 实现父类函数的调用，这里指的是 LeNet 调用父类 nn.Module 的初始化函数。\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "\n",
    "        out = F.relu(self.conv2(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "```\n",
    "\n",
    "## 1.2 模型初始化\n",
    "\n",
    "# 2. `nn.Module` 的详解\n",
    "\n",
    "## 2.1 `torch.nn`\n",
    "\n",
    "### 1). `nn.Module`\n",
    "> 所有网络层的基类，管理网络属性。\n",
    ">\n",
    "> 关于 `nn.Module` 的总结：\n",
    "> &emsp;&emsp; 1. 一个 `module` 可以包含多个子module；\n",
    ">\n",
    "> &emsp;&emsp; 2. 一个 `module` 相当于一个运算，必须实现 `forward()` 函数；\n",
    ">\n",
    "> &emsp;&emsp; 3. 每个 `module` 都有8个有序字典管理它的属性，重点关注 `modules` 和 `parameters`，管理模型和可学习参数；\n",
    "\n",
    "**重点关注：**\n",
    "\n",
    "**`parameters`:** `self._parameters = OrderDict()`，存储管理 `nn.Parameters` 类；\n",
    "\n",
    "**`modules`：**`self._modules = OrderDict()`，存储管理 `nn.Modules` 类；\n",
    "\n",
    "**`buffers`:** `self._buffers = OrderDict()`，存储管理缓冲属性，比如 BN层 中的均值、方差等；\n",
    "\n",
    "\n",
    "### 2). `nn.Parameter`\n",
    "> 张量(Tensor)子类，表示可学习参数，如 weight, bias。\n",
    "\n",
    "### 3). `nn.functional`\n",
    "函数具体实现，比如卷积、池化、激活函数等。\n",
    "\n",
    "### 4). `nn.init`\n",
    "参数初始化方法。\n",
    "\n",
    "# 3. 模型容器 (Containers) 以及 `AlexNet` 的构建\n",
    "```\n",
    "模型容器 (Containers)\n",
    "|--nn.Sequential\n",
    "|   |-- 按顺序 包装多个网络层。\n",
    "|   |-- 顺序性，各网络层之间严格按照顺序执行，常用语 block 构建。(整体上感觉 nn.Sequential 这个模型容器使用更方便)\n",
    "|--nn.MuduleList\n",
    "|   |-- 像 Python 的 list 一样包装多个网络层。\n",
    "|   |-- 迭代性，常用于大量重复网络构建。\n",
    "|--nn.ModuleDict\n",
    "|   |-- 像 Python 的 dict 一样包装多个网络层。\n",
    "|   |-- 索引性，常用语可选择的网络层。\n",
    "```\n",
    "\n",
    "## 3.1 Containers:\n",
    "\n",
    "### 1). `nn.Sequential`\n",
    "\n",
    "> 是 `nn.Module` 的容器，用于按顺序包装一组网络层。\n",
    ">\n",
    "> `nn.Sequential` 的特性：\n",
    "> \n",
    "> &emsp;&emsp; 1. 顺序性：各网络层之间按照严格的顺序进行构建。\n",
    ">\n",
    "> &emsp;&emsp; 2. 自带 `forward()`：在自带的`forward()`中，通过for循环一次执行前向传播。\n",
    "\n",
    "\n",
    "**示例：使用 `nn.Sequential` 构建 `LeNet`**\n",
    "\n",
    "```python\n",
    "## 方法1（网络层的名称使用默认的序号）;\n",
    "class LeNetSequential(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNetSequential, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU()\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 方法2（网络层的名称进行手动指定）\n",
    "class LeNetSequentialOrderDict(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNetSequentialOrderDict, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(OrderDict({\n",
    "            \"conv1\": nn.Conv2d(3, 6, 5),\n",
    "            \"relu1\": nn.ReLU(inplace=True),\n",
    "            \"pool1\": nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            \"conv2\": nn.Conv2d(6, 16, 5),\n",
    "            \"relu2\": nn.ReLU(inplace=True),\n",
    "            \"pool2\": nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        }))\n",
    "\n",
    "        self.classifier = nn.Sequential(OrderDict({\n",
    "            \"fc1\": nn.Linear(16*5*5, 120),\n",
    "            \"relu3\": nn.ReLU(inplace=True),\n",
    "            \"fc2\": nn.Linear(120, 84),\n",
    "            \"relu4\": nn.ReLU(inplace=True),\n",
    "            \"fc3\": nn.Linear(84, classes),\n",
    "        }))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "\n",
    "### 2). `nn.ModuleList`\n",
    "> 也是 `nn.Module` 的容器，用于包装一组网络层，并以迭代的方式调用网络层。\n",
    "> \n",
    "> 主要方法：\n",
    "> \n",
    "> `append()`：在 ModuleList 后面添加网络层；\n",
    ">\n",
    "> `extend()`: 拼接两个 ModuleList；\n",
    ">\n",
    "> `insert()`: 指定在 ModuleList 中特定位置插入网络层；\n",
    "\n",
    "\n",
    "**示例：用 `nn.ModuleList` 实现20个全连接层（每层有10个神经元）构成的网络**\n",
    "\n",
    "```python\n",
    "class FCModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCModuleList, self).__init__()\n",
    "        \n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(20)]) ## 用列表生成式进行构造\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "        return x\n",
    "\n",
    "```\n",
    "\n",
    "### 3. `nn.ModuleDict`\n",
    "> 也是 `nn.Module` 的容器，用于包装一组网络层，以索引的方式调用网络层。\n",
    ">\n",
    "> 主要方法：\n",
    ">\n",
    "> `clear()`: 清空 ModuleDict\n",
    ">\n",
    "> `items()`: 返回可迭代的 key-value\n",
    ">\n",
    "> `keys()`: 返回字典的 key\n",
    ">\n",
    "> `values()`: 返回字典的 value\n",
    ">\n",
    "> `pop()`: 返回一对 key-value，并从字典中删除\n",
    "\n",
    "\n",
    "**示例: Conv + Relu 的组合**\n",
    "```python\n",
    "class ModuleDict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleDict, self).__init__()\n",
    "        \n",
    "        self.choices = nn.ModuleDict({\n",
    "            \"conv\": nn.Conv2d(10, 10, 3),\n",
    "            \"pool\": nn.MaxPool2d(3)\n",
    "        })\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"prelu\": nn.PReLU()\n",
    "        })\n",
    "\n",
    "    def forward(self, choice, act):\n",
    "        x = self.choices[choice](x)\n",
    "        x = self.activations[act](x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "## 3.2 `AlexNet`的构建\n",
    "> `AlexNet`的特点：\n",
    ">\n",
    "> 1. 采用 ReLU 代替饱和激活函数(比如 Sigmoid)，减轻梯度消失；\n",
    ">\n",
    "> 2. 采用 LRN (Local Response Normalization)，对数据归一化，减轻梯度消失；\n",
    ">\n",
    "> 3. 采用 Dropout 提高全连接层的鲁棒性，增强网络的泛化能力；\n",
    ">\n",
    "> 4. 采用 Data Augmentation；\n",
    "\n",
    "**`AlexNet`的具体实现**\n",
    " \n",
    "```python\n",
    "## 来自于 Pytorch 官方提供的 AlexNet 实现 (torchvision.models.AlexNet())\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
